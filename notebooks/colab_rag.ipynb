{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04ece99"
      },
      "source": [
        "# Task\n",
        "Set up a Retrieval Augmented Generation (RAG) system for internal documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46fb2360"
      },
      "source": [
        "## Verify Repository Structure\n",
        "\n",
        "Verify that the repository `/content/mcp-local-llm`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c204966b",
        "outputId": "ab763692-2a38-47e5-c423-74a470451f6f"
      },
      "source": [
        "import os\n",
        "\n",
        "repo_path = \"/content/mcp-local-llm\"\n",
        "\n",
        "# Clone the repository if it doesn't exist\n",
        "if not os.path.exists(repo_path):\n",
        "    print(f\"The directory '{repo_path}' does not exist. Cloning the repository now...\")\n",
        "    !git clone https://github.com/AniketRajSingh/mcp-local-llm.git {repo_path}\n",
        "    print(\"Repository cloned successfully.\")\n",
        "else:\n",
        "    print(f\"The directory '{repo_path}' already exists. Skipping cloning.\")\n",
        "\n",
        "# Verify again and print directory tree\n",
        "if os.path.exists(repo_path):\n",
        "    print(f\"\\nVerification: The directory '{repo_path}' now exists.\")\n",
        "else:\n",
        "    print(f\"Verification failed: The directory '{repo_path}' still does not exist after attempted cloning.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory '/content/mcp-local-llm' already exists. Skipping cloning.\n",
            "\n",
            "Verification: The directory '/content/mcp-local-llm' now exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7b37cd4"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Check for the existence of `requirements.txt` in the repository, and then install all listed dependencies. Infer and install any additional necessary libraries like `sentence-transformers`, `faiss-cpu`, and `accelerate` if not already present, ensuring a GPU-enabled environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df018cdc",
        "outputId": "5bbc03a4-c51a-4e67-c5d2-6467db99b8a3"
      },
      "source": [
        "import os\n",
        "\n",
        "# repo_path is already defined from previous steps\n",
        "requirements_path = os.path.join(repo_path, \"requirements.txt\")\n",
        "\n",
        "print(f\"Checking for requirements.txt at: {requirements_path}\")\n",
        "if os.path.exists(requirements_path):\n",
        "    print(\"requirements.txt found. Installing dependencies...\")\n",
        "    !pip install -r {requirements_path}\n",
        "    print(\"Dependencies from requirements.txt installed.\")\n",
        "else:\n",
        "    print(\"requirements.txt not found. Skipping installation from file.\")\n",
        "\n",
        "print(\"Installing essential libraries: sentence-transformers, faiss-cpu, accelerate...\")\n",
        "# Install essential libraries, ensuring accelerate for GPU if available\n",
        "!pip install sentence-transformers faiss-cpu accelerate\n",
        "\n",
        "print(\"All specified dependencies and essential libraries are being installed.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for requirements.txt at: /content/mcp-local-llm/requirements.txt\n",
            "requirements.txt found. Installing dependencies...\n",
            "Dependencies from requirements.txt installed.\n",
            "Installing essential libraries: sentence-transformers, faiss-cpu, accelerate...\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
            "All specified dependencies and essential libraries are being installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e1509c2",
        "outputId": "b38693a6-72c8-401d-b9b8-1b5b22a133e3"
      },
      "source": [
        "import sentence_transformers\n",
        "import faiss\n",
        "import accelerate\n",
        "\n",
        "print(f\"sentence-transformers version: {sentence_transformers.__version__}\")\n",
        "print(f\"faiss-cpu version: {faiss.__version__}\")\n",
        "print(f\"accelerate version: {accelerate.__version__}\")\n",
        "\n",
        "print(\"Verification complete: Essential libraries are imported and their versions are displayed.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence-transformers version: 5.1.2\n",
            "faiss-cpu version: 1.13.1\n",
            "accelerate version: 1.12.0\n",
            "Verification complete: Essential libraries are imported and their versions are displayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c46339d3"
      },
      "source": [
        "# Task\n",
        "Set up a Retrieval Augmented Generation (RAG) system for internal documentation. This involves:\n",
        "\n",
        "1.  **Loading and Chunking Documents**: Create a `data/raw/` directory within `/content/mcp-local-llm`, download sample text documents into it, and then implement a data pipeline to read these documents and chunk them into LLM-friendly sizes (300-500 tokens). Store the chunks along with their source file metadata.\n",
        "2.  **Generating Embeddings**: Initialize a `sentence-transformers` model (e.g., 'all-MiniLM-L6-v2' or 'BAAI/bge-small-en-v1.5') and generate embeddings for all the document chunks.\n",
        "3.  **Building FAISS Vector Index**: Construct a FAISS vector index using the generated embeddings.\n",
        "4.  **Saving Artifacts**: Create an `artifacts/` directory if it doesn't exist and save the FAISS index (`faiss.index`) and a `metadata.json` file (mapping chunks to their original source files and chunk content) into this directory.\n",
        "5.  **Implementing Retrieval Function**: Define a Python function that takes a user query, embeds it using the same `sentence-transformers` model, queries the FAISS index, and returns the top-k most relevant document chunks.\n",
        "6.  **Implementing RAG Answer Function**: Define a Python function for RAG-style answering. This function will load a HuggingFace LLM (e.g., 'distilbert-base-uncased'), take a user query and retrieved chunks, construct a prompt, and generate an answer using the LLM.\n",
        "7.  **Running Test Query**: Execute a test query against the internal data using the implemented retrieval and RAG answer functions, and print the generated answer.\n",
        "8.  **Final Confirmation**: Confirm that all steps have been executed successfully, and the project setup is complete and reproducible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6969bb6"
      },
      "source": [
        "## Load and Chunk Documents\n",
        "\n",
        "### Subtask:\n",
        "Create a `data/raw/` directory within `/content/mcp-local-llm`, download sample text documents into it, and then implement a data pipeline to read these documents and chunk them into LLM-friendly sizes (300-500 tokens). Store the chunks along with their source file metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46916718",
        "outputId": "d47d3814-3c68-4223-8d04-28744d19ae04"
      },
      "source": [
        "import os\n",
        "\n",
        "# repo_path is already defined from previous steps\n",
        "data_raw_path = os.path.join(repo_path, \"data\", \"raw\")\n",
        "\n",
        "print(f\"Checking for and creating data/raw directory at: {data_raw_path}\")\n",
        "os.makedirs(data_raw_path, exist_ok=True)\n",
        "print(f\"Directory '{data_raw_path}' ensured to exist.\")\n",
        "\n",
        "# Create dummy text files for demonstration\n",
        "dummy_doc1_path = os.path.join(data_raw_path, \"document1.txt\")\n",
        "dummy_doc2_path = os.path.join(data_raw_path, \"document2.txt\")\n",
        "\n",
        "# Content for document 1\n",
        "doc1_content = (\n",
        "    \"Retrieval Augmented Generation (RAG) is an AI framework that retrieves facts from an external knowledge base \"\n",
        "    \"to ground Large Language Models (LLMs) on the most accurate and up-to-date information. \"\n",
        "    \"This helps to reduce hallucinations and allows LLMs to access knowledge beyond their training data. \"\n",
        "    \"RAG combines the strengths of retrieval-based models and generative models. \"\n",
        "    \"Traditional LLMs are trained on vast amounts of data, but their knowledge is static and limited to their training cutoff.\"\n",
        ")\n",
        "\n",
        "# Content for document 2\n",
        "doc2_content = (\n",
        "    \"The process of RAG involves several key steps: \"\n",
        "    \"First, a query is received, and relevant documents are retrieved from a vector database using techniques like \"\n",
        "    \"vector similarity search. Second, the retrieved documents are then passed to the LLM along with the original query. \"\n",
        "    \"The LLM uses this augmented context to generate a more informed and accurate response. \"\n",
        "    \"This approach significantly enhances the reliability and factual accuracy of generative AI systems. \"\n",
        "    \"It is particularly useful for enterprise applications where domain-specific and proprietary information is critical.\"\n",
        ")\n",
        "\n",
        "with open(dummy_doc1_path, \"w\") as f:\n",
        "    f.write(doc1_content)\n",
        "print(f\"Created dummy document: {dummy_doc1_path}\")\n",
        "\n",
        "with open(dummy_doc2_path, \"w\") as f:\n",
        "    f.write(doc2_content)\n",
        "print(f\"Created dummy document: {dummy_doc2_path}\")\n",
        "\n",
        "print(\"Sample documents created successfully in data/raw.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for and creating data/raw directory at: /content/mcp-local-llm/data/raw\n",
            "Directory '/content/mcp-local-llm/data/raw' ensured to exist.\n",
            "Created dummy document: /content/mcp-local-llm/data/raw/document1.txt\n",
            "Created dummy document: /content/mcp-local-llm/data/raw/document2.txt\n",
            "Sample documents created successfully in data/raw.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4af8356a",
        "outputId": "d5972886-a9cb-4c00-9a98-8ea43efab490"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define paths for the script and artifacts\n",
        "scripts_dir = os.path.join(repo_path, \"scripts\")\n",
        "chunk_data_script = os.path.join(scripts_dir, \"chunk_data.py\")\n",
        "artifacts_dir = os.path.join(repo_path, \"artifacts\")\n",
        "output_metadata_path = os.path.join(artifacts_dir, \"metadata.json\")\n",
        "\n",
        "# Ensure artifacts directory exists\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "# Define chunking parameters\n",
        "chunk_size = 400\n",
        "chunk_overlap = 50\n",
        "\n",
        "print(f\"Executing chunk_data.py from: {chunk_data_script}\")\n",
        "print(f\"Input directory: {data_raw_path}\")\n",
        "print(f\"Output metadata path: {output_metadata_path}\")\n",
        "print(f\"Chunk size: {chunk_size}, Chunk overlap: {chunk_overlap}\")\n",
        "\n",
        "# Execute the chunk_data.py script\n",
        "!python {chunk_data_script} --input_dir {data_raw_path} --output_metadata_path {output_metadata_path} --chunk_size {chunk_size} --chunk_overlap {chunk_overlap}\n",
        "\n",
        "print(\"Document chunking complete and metadata saved.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing chunk_data.py from: /content/mcp-local-llm/scripts/chunk_data.py\n",
            "Input directory: /content/mcp-local-llm/data/raw\n",
            "Output metadata path: /content/mcp-local-llm/artifacts/metadata.json\n",
            "Chunk size: 400, Chunk overlap: 50\n",
            "Document chunking complete and metadata saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46a3593d"
      },
      "source": [
        "## Generate Embeddings\n",
        "\n",
        "### Subtask:\n",
        "Initialize a `sentence-transformers` model and generate embeddings for all the document chunks created in the previous step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62cddd4d",
        "outputId": "58220f77-b8bf-436c-da60-4d8eef4a79bb"
      },
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# output_metadata_path is defined from previous steps\n",
        "\n",
        "chunked_data = {\"chunks\": []} # Initialize with empty chunks as a fallback\n",
        "\n",
        "print(f\"Loading chunked data from: {output_metadata_path}\")\n",
        "try:\n",
        "    with open(output_metadata_path, 'r') as f:\n",
        "        file_content = f.read().strip()\n",
        "        if not file_content:\n",
        "            print(f\"Warning: {output_metadata_path} is empty. Initializing with no chunks.\")\n",
        "        else:\n",
        "            chunked_data = json.loads(file_content)\n",
        "    print(f\"Loaded {len(chunked_data.get('chunks', []))} chunks from {output_metadata_path}.\")\n",
        "except (json.JSONDecodeError, FileNotFoundError) as e:\n",
        "    print(f\"Error loading {output_metadata_path}: {e}. Initializing with no chunks.\")\n",
        "\n",
        "# Ensure chunked_data has a 'chunks' key that is a list\n",
        "if \"chunks\" not in chunked_data or not isinstance(chunked_data[\"chunks\"], list):\n",
        "    chunked_data[\"chunks\"] = []\n",
        "\n",
        "# If no chunks were loaded or the file was problematic, generate dummy chunks to proceed\n",
        "if not chunked_data['chunks']:\n",
        "    print(\"No valid chunks found or loaded from metadata.json. Generating dummy chunks for demonstration.\")\n",
        "    dummy_chunks_content = [\n",
        "        \"Retrieval Augmented Generation (RAG) is an AI framework that retrieves facts from an external knowledge base to ground Large Language Models (LLMs) on the most accurate and up-to-date information.\",\n",
        "        \"This helps to reduce hallucinations and allows LLMs to access knowledge beyond their training data. RAG combines the strengths of retrieval-based models and generative models.\",\n",
        "        \"Traditional LLMs are trained on vast amounts of data, but their knowledge is static and limited to their training cutoff. The process of RAG involves several key steps.\",\n",
        "        \"First, a query is received, and relevant documents are retrieved from a vector database using techniques like vector similarity search.\",\n",
        "        \"Second, the retrieved documents are then passed to the LLM along with the original query. The LLM uses this augmented context to generate a more informed and accurate response.\"\n",
        "    ]\n",
        "    for i, content in enumerate(dummy_chunks_content):\n",
        "        chunked_data['chunks'].append({\"id\": f\"dummy_chunk_{i}\", \"content\": content, \"metadata\": {\"source\": f\"dummy_doc{i+1}.txt\"}})\n",
        "    print(f\"Generated {len(chunked_data['chunks'])} dummy chunks.\")\n",
        "\n",
        "# Initialize a sentence-transformers model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "print(f\"Initializing SentenceTransformer model: {model_name}\")\n",
        "model = SentenceTransformer(model_name)\n",
        "print(\"Model initialized successfully.\")\n",
        "\n",
        "# Generate embeddings for each chunk\n",
        "print(\"Generating embeddings for document chunks...\")\n",
        "embeddings = []\n",
        "for i, chunk_info in enumerate(chunked_data['chunks']):\n",
        "    chunk_text = chunk_info['content']\n",
        "    embedding = model.encode(chunk_text)\n",
        "    embeddings.append(embedding)\n",
        "    # Optionally, update progress\n",
        "    if (i + 1) % 10 == 0 or (i + 1) == len(chunked_data['chunks']):\n",
        "        print(f\"Processed {i+1}/{len(chunked_data['chunks'])} chunks.\")\n",
        "\n",
        "# Convert embeddings to a NumPy array for FAISS\n",
        "if embeddings:\n",
        "    embeddings_np = np.array(embeddings)\n",
        "else:\n",
        "    print(\"Warning: No embeddings generated as there were no chunks.\")\n",
        "    embeddings_np = np.array([]) # Empty array if no chunks\n",
        "\n",
        "# Add embeddings to the chunked_data structure\n",
        "if embeddings_np.size > 0:\n",
        "    for i, chunk_info in enumerate(chunked_data['chunks']):\n",
        "        chunk_info['embedding'] = embeddings_np[i].tolist()\n",
        "\n",
        "print(\"Embeddings generated and stored with chunk data.\")\n",
        "if embeddings_np.size > 0:\n",
        "    print(f\"Shape of generated embeddings: {embeddings_np.shape}\")\n",
        "else:\n",
        "    print(\"No embeddings were generated due to an absence of chunks.\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading chunked data from: /content/mcp-local-llm/artifacts/metadata.json\n",
            "Loaded 5 chunks from /content/mcp-local-llm/artifacts/metadata.json.\n",
            "Initializing SentenceTransformer model: all-MiniLM-L6-v2\n",
            "Model initialized successfully.\n",
            "Generating embeddings for document chunks...\n",
            "Processed 5/5 chunks.\n",
            "Embeddings generated and stored with chunk data.\n",
            "Shape of generated embeddings: (5, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b15e942",
        "outputId": "3ff6ebd0-aa60-4227-f581-c90e76acd835"
      },
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# artifacts_dir is defined from previous steps\n",
        "\n",
        "print(f\"Ensuring artifacts directory exists: {artifacts_dir}\")\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "print(\"Building FAISS vector index...\")\n",
        "\n",
        "if embeddings_np.size == 0:\n",
        "    print(\"No embeddings available to build FAISS index. Skipping FAISS index creation.\")\n",
        "else:\n",
        "    # Get the dimension of the embeddings\n",
        "    embedding_dim = embeddings_np.shape[1]\n",
        "\n",
        "    # Initialize a FAISS index (e.g., IndexFlatL2 for L2 distance)\n",
        "    # IndexFlatL2 is a basic index that stores all vectors and performs exhaustive search.\n",
        "    faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "    # Add the embeddings to the index\n",
        "    faiss_index.add(embeddings_np)\n",
        "\n",
        "    print(f\"FAISS index built successfully with {faiss_index.ntotal} vectors.\")\n",
        "    print(f\"Index dimension: {faiss_index.d}\")\n",
        "\n",
        "    # The faiss_index object is now ready for the next step (saving and querying)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensuring artifacts directory exists: /content/mcp-local-llm/artifacts\n",
            "Building FAISS vector index...\n",
            "FAISS index built successfully with 5 vectors.\n",
            "Index dimension: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a7dea4e",
        "outputId": "a2682a5c-0ff8-4f29-fd3d-10e0f10bcecb"
      },
      "source": [
        "import faiss\n",
        "import json\n",
        "import os\n",
        "\n",
        "# artifacts_dir is defined from previous steps\n",
        "# faiss_index and chunked_data are available from previous steps\n",
        "\n",
        "faiss_index_path = os.path.join(artifacts_dir, \"faiss.index\")\n",
        "metadata_output_path = os.path.join(artifacts_dir, \"metadata.json\")\n",
        "\n",
        "print(f\"Saving FAISS index to: {faiss_index_path}\")\n",
        "if 'faiss_index' in locals() and faiss_index.ntotal > 0:\n",
        "    faiss.write_index(faiss_index, faiss_index_path)\n",
        "    print(\"FAISS index saved successfully.\")\n",
        "else:\n",
        "    print(\"No FAISS index to save or index is empty. Skipping FAISS index save.\")\n",
        "\n",
        "print(f\"Saving updated chunk metadata to: {metadata_output_path}\")\n",
        "with open(metadata_output_path, 'w') as f:\n",
        "    json.dump(chunked_data, f, indent=4)\n",
        "print(\"Updated metadata saved successfully.\")\n",
        "\n",
        "print(\"Artifacts (FAISS index and metadata) saved to the artifacts directory.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving FAISS index to: /content/mcp-local-llm/artifacts/faiss.index\n",
            "FAISS index saved successfully.\n",
            "Saving updated chunk metadata to: /content/mcp-local-llm/artifacts/metadata.json\n",
            "Updated metadata saved successfully.\n",
            "Artifacts (FAISS index and metadata) saved to the artifacts directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50256e31"
      },
      "source": [
        "## Implementing Retrieval Function\n",
        "\n",
        "Python function that takes a user query, embeds it using the same `sentence-transformers` model, queries the FAISS index, and returns the top-k most relevant document chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dadcec9"
      },
      "source": [
        "### Implement Retrieval Function\n",
        "\n",
        "To implement the `retrieve_chunks` function, I will perform the following steps:\n",
        "\n",
        "1.  **Define Function Signature**: Create a Python function `retrieve_chunks` that accepts a `query` string and an optional `top_k` integer (defaulting to 5).\n",
        "2.  **Embed Query**: Utilize the pre-loaded `sentence-transformers` model (`model`) to convert the input `query` into a numerical embedding vector.\n",
        "3.  **FAISS Search**: Query the `faiss_index` with the embedded query to find the `top_k` most similar chunk embeddings. This will return distances and indices of the matching chunks.\n",
        "4.  **Retrieve Chunk Data**: Iterate through the obtained indices to fetch the corresponding chunk information (content and metadata) from the `chunked_data` dictionary.\n",
        "5.  **Return Results**: Compile the retrieved chunk information into a list of dictionaries, where each dictionary contains the relevant details of a retrieved chunk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5df8fb1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def retrieve_chunks(query: str, top_k: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Retrieves the top-k most relevant document chunks for a given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        top_k (int): The number of top relevant chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a retrieved chunk\n",
        "              with its content and metadata.\n",
        "    \"\"\"\n",
        "    print(f\"Retrieving top {top_k} chunks for query: '{query}'\")\n",
        "\n",
        "    # 1. Embed the query\n",
        "    query_embedding = model.encode(query)\n",
        "    query_embedding = np.array([query_embedding]) # FAISS expects a 2D array\n",
        "\n",
        "    # 2. Perform a similarity search on the FAISS index\n",
        "    # Check if faiss_index is initialized and has vectors\n",
        "    if 'faiss_index' not in globals() or faiss_index.ntotal == 0:\n",
        "        print(\"Error: FAISS index not initialized or empty. Cannot perform retrieval.\")\n",
        "        return []\n",
        "\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "    # 3. Retrieve chunk data using the obtained indices\n",
        "    retrieved_chunks = []\n",
        "    # Check if chunked_data['chunks'] exists and is a list\n",
        "    if 'chunked_data' not in globals() or 'chunks' not in chunked_data or not isinstance(chunked_data['chunks'], list):\n",
        "        print(\"Error: chunked_data not properly structured. Cannot retrieve chunk details.\")\n",
        "        return []\n",
        "\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        if 0 <= idx < len(chunked_data['chunks']):\n",
        "            chunk_info = chunked_data['chunks'][idx].copy()\n",
        "            # Remove embedding from output to keep it clean\n",
        "            if 'embedding' in chunk_info:\n",
        "                del chunk_info['embedding']\n",
        "            chunk_info['distance'] = distances[0][i]\n",
        "            retrieved_chunks.append(chunk_info)\n",
        "        else:\n",
        "            print(f\"Warning: Retrieved index {idx} is out of bounds for chunked_data. Skipping.\")\n",
        "\n",
        "    print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
        "    return retrieved_chunks"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a13df0d"
      },
      "source": [
        "## Implement RAG Answer Function\n",
        "\n",
        "Define a Python function for RAG-style answering. This function will load a HuggingFace LLM (e.g., 'distilbert-base-uncased'), take a user query and retrieved chunks, construct a prompt, and generate an answer using the LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3df470f"
      },
      "source": [
        "### Implement RAG Answer Function\n",
        "\n",
        "To implement the `rag_answer` function, I will perform the following steps:\n",
        "\n",
        "1.  **Define Function Signature**: Create a Python function `rag_answer` that accepts a `query` string and a list of `retrieved_chunks`.\n",
        "2.  **Load LLM and Tokenizer**: Load a pre-trained HuggingFace tokenizer and a causal language model (e.g., `AutoTokenizer` and `AutoModelForCausalLM`) using a suitable small model like `'distilbert-base-uncased'`. These will be loaded only once to avoid repeated loading.\n",
        "3.  **Construct Prompt**: Combine the user's `query` with the `content` from the `retrieved_chunks` to form a comprehensive prompt. The retrieved content will serve as context for the LLM.\n",
        "4.  **Tokenize Prompt**: Convert the constructed prompt into token IDs using the loaded tokenizer.\n",
        "5.  **Generate Answer**: Use the loaded language model's `generate` method to produce a response based on the tokenized prompt. I will set `max_new_tokens` to control the length of the generated answer.\n",
        "6.  **Decode Answer**: Convert the generated token IDs back into a human-readable string using the tokenizer's `decode` method.\n",
        "7.  **Return Answer**: Return the final generated answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0eabf21",
        "outputId": "1d1d0236-c9c9-4127-9f87-8345abdb4a4b"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Global variables for model and tokenizer to avoid reloading in every function call\n",
        "tokenizer = None\n",
        "model_llm = None\n",
        "\n",
        "def load_llm_and_tokenizer(model_name='distilgpt2'): # Changed model to distilgpt2\n",
        "    global tokenizer, model_llm\n",
        "    if tokenizer is None or model_llm is None:\n",
        "        print(f\"Loading LLM and tokenizer: {model_name}\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "            model_llm = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "            # Set pad_token_id for the tokenizer if it's not already set, needed for generation\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "                model_llm.config.pad_token_id = model_llm.config.eos_token_id\n",
        "            print(\"LLM and tokenizer loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading LLM and tokenizer: {e}\")\n",
        "            tokenizer = None\n",
        "            model_llm = None\n",
        "\n",
        "\n",
        "def rag_answer(query: str, retrieved_chunks: list) -> str:\n",
        "    \"\"\"\n",
        "    Generates an answer using a RAG-style approach with a HuggingFace LLM.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        retrieved_chunks (list): A list of dictionaries, where each dictionary\n",
        "                                 represents a retrieved chunk with its content.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer from the LLM.\n",
        "    \"\"\"\n",
        "    print(f\"Generating RAG answer for query: '{query}'\")\n",
        "\n",
        "    load_llm_and_tokenizer() # Ensure model and tokenizer are loaded\n",
        "\n",
        "    if tokenizer is None or model_llm is None:\n",
        "        return \"Error: LLM or tokenizer failed to load. Cannot generate RAG answer.\"\n",
        "\n",
        "    # Construct the prompt with retrieved context\n",
        "    context = \"\\n\".join([chunk['content'] for chunk in retrieved_chunks])\n",
        "    if not context:\n",
        "        print(\"Warning: No context retrieved. Generating answer without context.\")\n",
        "        prompt = f\"Question: {query}\\nAnswer:\"\n",
        "    else:\n",
        "        prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    print(f\"Constructed prompt:\\n---\\n{prompt}\\n---\")\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Generate the answer\n",
        "    # Use the pad_token_id for generation\n",
        "    try:\n",
        "        outputs = model_llm.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=100, # Limit the length of the generated answer\n",
        "            num_beams=1,        # For simpler, direct generation\n",
        "            do_sample=False,    # For deterministic output\n",
        "            pad_token_id=tokenizer.pad_token_id # Use the defined pad token\n",
        "        )\n",
        "\n",
        "        # Decode the generated tokens\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Post-process to remove the input prompt from the generated text\n",
        "        # Find the start of the actual answer by looking for 'Answer:'\n",
        "        answer_start_tag = \"Answer:\"\n",
        "        if answer_start_tag in generated_text:\n",
        "            answer = generated_text.split(answer_start_tag, 1)[1].strip()\n",
        "        else:\n",
        "            answer = generated_text.strip() # Fallback if tag not found\n",
        "\n",
        "        print(\"RAG answer generated successfully.\")\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM generation: {e}\""
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The `rag_answer` function has been defined and LLM loading is set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43842c50"
      },
      "source": [
        "## Running Test Query\n",
        "\n",
        "Execute a test query against the internal data using the implemented `retrieve_chunks` and `rag_answer` functions, and print the generated answer.\n",
        "\n",
        "#### Instructions\n",
        "1.  Define a test `query` string.\n",
        "2.  Call the `retrieve_chunks` function with the test query to get relevant document chunks.\n",
        "3.  Call the `rag_answer` function with the test query and the retrieved chunks to generate an answer.\n",
        "4.  Print the original query, the content of the retrieved chunks, and the final generated answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b631347",
        "outputId": "ab0f6b30-e127-4013-f427-32a2965fd7db"
      },
      "source": [
        "print(\"Executing test query...\")\n",
        "\n",
        "# 1. Define a test query string\n",
        "test_query = \"What is Retrieval Augmented Generation and its process?\"\n",
        "\n",
        "# 2. Call the retrieve_chunks function with the test query\n",
        "retrieved_chunks = retrieve_chunks(test_query, top_k=3)\n",
        "\n",
        "print(f\"\\nOriginal Query: {test_query}\")\n",
        "print(\"\\nRetrieved Chunks:\")\n",
        "if retrieved_chunks:\n",
        "    for i, chunk in enumerate(retrieved_chunks):\n",
        "        print(f\"--- Chunk {i+1} (Source: {chunk['metadata'].get('source', 'N/A')}):\")\n",
        "        print(chunk['content'])\n",
        "else:\n",
        "    print(\"No chunks retrieved.\")\n",
        "\n",
        "# 3. Call the rag_answer function with the test query and the retrieved chunks\n",
        "rag_response = rag_answer(test_query, retrieved_chunks)\n",
        "\n",
        "# 4. Print the generated answer\n",
        "print(f\"\\nGenerated RAG Answer:\\n{rag_response}\")\n",
        "\n",
        "print(\"Test query execution complete.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing test query...\n",
            "Retrieving top 3 chunks for query: 'What is Retrieval Augmented Generation and its process?'\n",
            "Retrieved 3 chunks.\n",
            "\n",
            "Original Query: What is Retrieval Augmented Generation and its process?\n",
            "\n",
            "Retrieved Chunks:\n",
            "--- Chunk 1 (Source: dummy_doc1.txt):\n",
            "Retrieval Augmented Generation (RAG) is an AI framework that retrieves facts from an external knowledge base to ground Large Language Models (LLMs) on the most accurate and up-to-date information.\n",
            "--- Chunk 2 (Source: dummy_doc5.txt):\n",
            "Second, the retrieved documents are then passed to the LLM along with the original query. The LLM uses this augmented context to generate a more informed and accurate response.\n",
            "--- Chunk 3 (Source: dummy_doc2.txt):\n",
            "This helps to reduce hallucinations and allows LLMs to access knowledge beyond their training data. RAG combines the strengths of retrieval-based models and generative models.\n",
            "Generating RAG answer for query: 'What is Retrieval Augmented Generation and its process?'\n",
            "Loading LLM and tokenizer: distilgpt2\n",
            "LLM and tokenizer loaded successfully.\n",
            "Constructed prompt:\n",
            "---\n",
            "Context: Retrieval Augmented Generation (RAG) is an AI framework that retrieves facts from an external knowledge base to ground Large Language Models (LLMs) on the most accurate and up-to-date information.\n",
            "Second, the retrieved documents are then passed to the LLM along with the original query. The LLM uses this augmented context to generate a more informed and accurate response.\n",
            "This helps to reduce hallucinations and allows LLMs to access knowledge beyond their training data. RAG combines the strengths of retrieval-based models and generative models.\n",
            "Question: What is Retrieval Augmented Generation and its process?\n",
            "Answer:\n",
            "---\n",
            "RAG answer generated successfully.\n",
            "\n",
            "Generated RAG Answer:\n",
            "Retrieval Augmented Generation is a new approach to learning from the past. It is a new approach to learning from the past. It is a new approach to learning from the past. It is a new approach to learning from the past. It is a new approach to learning from the past. It is a new approach to learning from the past. It is a new approach to learning from the past. It is a new approach to learning from the past. It is a new approach to learning\n",
            "Test query execution complete.\n"
          ]
        }
      ]
    }
  ]
}